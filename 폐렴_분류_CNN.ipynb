{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bq4BJRa3KUcl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from skimage.io import imread\n",
    "from keras.utils import np_utils\n",
    "import os\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIArHWBxzsXg"
   },
   "outputs": [],
   "source": [
    "seed=32152339\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guqAy8xCwBwL"
   },
   "outputs": [],
   "source": [
    "print('tensorflow version :',tf.__version__)\n",
    "print('keras versoin :',tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6Ej5Z5NLRem"
   },
   "outputs": [],
   "source": [
    "drive.mount('/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yDVgfLhUzLd"
   },
   "outputs": [],
   "source": [
    "# Define path to the data directory\n",
    "data_dir = Path('/gdrive/MyDrive/Colab Notebooks/kaggle_pneumonia_classification')\n",
    "\n",
    "# Path to train directory (Fancy pathlib...no more os.path!!)\n",
    "train_dir = data_dir / 'train'\n",
    "\n",
    "# Path to validation directory\n",
    "val_dir = data_dir / 'val'\n",
    "\n",
    "# Path to test directory\n",
    "test_dir = data_dir / 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T57_3JIMXM3z"
   },
   "outputs": [],
   "source": [
    "# train path 적재\n",
    "normal_cases_dir = train_dir / 'NORMAL'\n",
    "pneumonia_cases_dir = train_dir / 'PNEUMONIA'\n",
    "normal_cases = normal_cases_dir.glob('*.jpeg')\n",
    "pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n",
    "\n",
    "# 적재할 리스트 생성\n",
    "train_data = []\n",
    "\n",
    "# 정상 이미지와 라벨 적재\n",
    "for img in normal_cases:\n",
    "    train_data.append((img,0))\n",
    "\n",
    "# 폐렴 이미지와 라벨 적재\n",
    "for img in pneumonia_cases:\n",
    "    train_data.append((img, 1))\n",
    "\n",
    "# 데이터 프레임으로 변환\n",
    "train_data = pd.DataFrame(train_data, columns=['image', 'label'],index=None)\n",
    "\n",
    "# 데이터 섞기 \n",
    "train_data = train_data.sample(frac=1.).reset_index(drop=True)\n",
    "\n",
    "# 데이터 프레임 살펴보기\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLo8AApVcHxT"
   },
   "outputs": [],
   "source": [
    "img_size=224\n",
    "X_train = np.zeros(shape=(len(train_data),img_size,img_size,3))\n",
    "\n",
    "for idx,fname in enumerate(tqdm(train_data.image)):\n",
    "  img = image.load_img(fname,target_size=(img_size,img_size))\n",
    "  img_array_train = image.img_to_array(img)\n",
    "  img_array_train = np.expand_dims(img_array_train,axis=0)\n",
    "  X_train[idx] = img_array_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiEO3NzobEDb"
   },
   "outputs": [],
   "source": [
    "# valid path 적재\n",
    "normal_cases_dir = val_dir / 'NORMAL'\n",
    "pneumonia_cases_dir = val_dir / 'PNEUMONIA'\n",
    "normal_cases = normal_cases_dir.glob('*.jpeg')\n",
    "pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n",
    "\n",
    "# 적재할 리스트 생성\n",
    "val_data = []\n",
    "\n",
    "# 정상 이미지와 라벨 적재\n",
    "for img in normal_cases:\n",
    "    val_data.append((img,0))\n",
    "\n",
    "# 폐렴 이미지와 라벨 적재\n",
    "for img in pneumonia_cases:\n",
    "    val_data.append((img, 1))\n",
    "\n",
    "# 데이터 프레임으로 변환\n",
    "val_data = pd.DataFrame(val_data, columns=['image', 'label'],index=None)\n",
    "\n",
    "# 데이터 섞기 \n",
    "val_data = val_data.sample(frac=1.).reset_index(drop=True)\n",
    "\n",
    "# 데이터 프레임 살펴보기\n",
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuDj6Cz9gnKd"
   },
   "outputs": [],
   "source": [
    "img_size=224\n",
    "X_val = np.zeros(shape=(len(val_data),img_size,img_size,3))\n",
    "\n",
    "for idx,fname in enumerate(tqdm(val_data.image)):\n",
    "  img = image.load_img(fname,target_size=(img_size,img_size))\n",
    "  img_array_val = image.img_to_array(img)\n",
    "  img_array_val = np.expand_dims(img_array_val,axis=0)\n",
    "  X_val[idx] = img_array_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MIU-z3IgxZH"
   },
   "outputs": [],
   "source": [
    "# test path 적재\n",
    "normal_cases_dir = test_dir / 'NORMAL'\n",
    "pneumonia_cases_dir = test_dir / 'PNEUMONIA'\n",
    "normal_cases = normal_cases_dir.glob('*.jpeg')\n",
    "pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n",
    "\n",
    "# 적재할 리스트 생성\n",
    "test_data = []\n",
    "\n",
    "# 정상 이미지와 라벨 적재\n",
    "for img in normal_cases:\n",
    "    test_data.append((img,0))\n",
    "\n",
    "# 폐렴 이미지와 라벨 적재\n",
    "for img in pneumonia_cases:\n",
    "    test_data.append((img, 1))\n",
    "\n",
    "# 데이터 프레임으로 변환\n",
    "test_data = pd.DataFrame(test_data, columns=['image', 'label'],index=None)\n",
    "\n",
    "# 데이터 섞기 \n",
    "test_data = test_data.sample(frac=1.).reset_index(drop=True)\n",
    "\n",
    "# 데이터 프레임 살펴보기\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXj8707XgxSs"
   },
   "outputs": [],
   "source": [
    "img_size=224\n",
    "X_test = np.zeros(shape=(len(test_data),img_size,img_size,3))\n",
    "\n",
    "for idx,fname in enumerate(tqdm(test_data.image)):\n",
    "  img = image.load_img(fname,target_size=(img_size,img_size))\n",
    "  img_array_test = image.img_to_array(img)\n",
    "  img_array_test = np.expand_dims(img_array_test,axis=0)\n",
    "  X_test[idx] = img_array_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knBL_zLvgxKF"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k30oCInJmPO7"
   },
   "outputs": [],
   "source": [
    "print('train set shape :',X_train.shape,train_data['label'].shape)\n",
    "print('valid set shape :',X_val.shape,val_data['label'].shape)\n",
    "print('test set shape :',X_test.shape,test_data['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCt2tj0xmPKm"
   },
   "outputs": [],
   "source": [
    "plt.bar([\"Pneumonia : 1\",\"Normal : 0\"],train_data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3l3B5YPm5nP"
   },
   "source": [
    "- 폐렴이미지의 수가 3배정도 높은 빈도를 관찰할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_iLAWEXqQ9I"
   },
   "source": [
    "#### train image 관찰하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wq7qQFFKmPFp"
   },
   "outputs": [],
   "source": [
    "#train image 관찰하기\n",
    "pneumonia_samples = (train_data[train_data['label']==1]['image'].iloc[:5]).tolist()\n",
    "normal_samples = (train_data[train_data['label']==0]['image'].iloc[:5]).tolist()\n",
    "# 폐렴과 정상 이미지 통합\n",
    "samples = pneumonia_samples + normal_samples\n",
    "del pneumonia_samples, normal_samples\n",
    "# 시각화\n",
    "f, ax = plt.subplots(2,5, figsize=(30,10))\n",
    "for i in range(10):\n",
    "    img = imread(samples[i])\n",
    "    ax[i//5, i%5].imshow(img, cmap='gray')\n",
    "    if i<5:\n",
    "        ax[i//5, i%5].set_title(\"Pneumonia\")\n",
    "    else:\n",
    "        ax[i//5, i%5].set_title(\"Normal\")\n",
    "    ax[i//5, i%5].axis('off')\n",
    "    ax[i//5, i%5].set_aspect('auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri7e9QWRmPBc"
   },
   "source": [
    "- 육안상으로 폐렴 유무를 구분하기 힘듬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msiCM23mqaW6"
   },
   "source": [
    "#### valid image 관찰하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTPoqSbrqXof"
   },
   "outputs": [],
   "source": [
    "#valid image 관찰하기\n",
    "pneumonia_samples = (val_data[train_data['label']==1]['image'].iloc[:5]).tolist()\n",
    "normal_samples = (val_data[train_data['label']==0]['image'].iloc[:5]).tolist()\n",
    "\n",
    "# 폐렴과 정상 이미지 통합\n",
    "samples = pneumonia_samples + normal_samples\n",
    "del pneumonia_samples, normal_samples\n",
    "\n",
    "# 시각화\n",
    "f, ax = plt.subplots(2,5, figsize=(30,10))\n",
    "for i in range(10):\n",
    "    img = imread(samples[i])\n",
    "    ax[i//5, i%5].imshow(img, cmap='gray')\n",
    "    if i<5:\n",
    "        ax[i//5, i%5].set_title(\"Pneumonia\")\n",
    "    else:\n",
    "        ax[i//5, i%5].set_title(\"Normal\")\n",
    "    ax[i//5, i%5].axis('off')\n",
    "    ax[i//5, i%5].set_aspect('auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rUEqGuHrZSE"
   },
   "source": [
    "이미지의 특징\n",
    "- 가운데로 모두 정렬되어 있기 때문에 augmentaion 큰 변화 지양\n",
    "- 이미지의 좌우상하가 명확함 \n",
    "- 이미지의 각도가 일정함\n",
    "- 흑백이며 x-ray이기 때문에 밝기도 어느정도 일정함.\n",
    "\n",
    "=> 이러한 정보를 바탕으로 augmentaion 시도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3iNeoHkELwq"
   },
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9FwYmfztmHJn"
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rescale = 1./255,\n",
    "        featurewise_center=False, \n",
    "        samplewise_center=False,  \n",
    "        featurewise_std_normalization=False,  \n",
    "        samplewise_std_normalization=False,  \n",
    "        zca_whitening=False, \n",
    "        rotation_range = 30, \n",
    "        zoom_range = 0.2, \n",
    "        width_shift_range=0.1,  \n",
    "        height_shift_range=0.1,  \n",
    "        horizontal_flip = False,  \n",
    "        vertical_flip=False) \n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xz7XxvXqyH0v"
   },
   "source": [
    "- 30도씩 랜덤하게 회전\n",
    "- 무작위로 20% 확대/축소\n",
    "- 10%만큼 수평으로 이동\n",
    "- 10%만큼 수직으로 이동\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZhM9rmEmOdx"
   },
   "outputs": [],
   "source": [
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axVrEcAhmFV2"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mYw7iJtgw_N"
   },
   "outputs": [],
   "source": [
    "# CNN 빌드\n",
    "def conv_block(filters):\n",
    "    block = tf.keras.Sequential([\n",
    "        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.SeparableConv2D(filters, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPool2D()\n",
    "    ]\n",
    "    )\n",
    "    return block\n",
    "def dense_block(units, dropout_rate):\n",
    "    block = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units, activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    return block\n",
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(img_size, img_size, 3)),\n",
    "        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.Conv2D(16, 3, activation='relu', padding='same'),\n",
    "        tf.keras.layers.MaxPool2D(),       \n",
    "        conv_block(32),\n",
    "        conv_block(64),        \n",
    "        conv_block(128),\n",
    "        tf.keras.layers.Dropout(0.2),       \n",
    "        conv_block(256),\n",
    "        tf.keras.layers.Dropout(0.2),       \n",
    "        tf.keras.layers.Flatten(),\n",
    "        dense_block(512, 0.7),\n",
    "        dense_block(128, 0.5),\n",
    "        dense_block(64, 0.3),        \n",
    "        tf.keras.layers.Dense(2, activation='softmax')  \n",
    "    ])\n",
    "    model.compile( loss='binary_crossentropy',optimizer='adam',metrics='accuracy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NbFN89hDABhs"
   },
   "outputs": [],
   "source": [
    "# 모델 밸런싱\n",
    "\n",
    "train_data['label'].value_counts(normalize=True)[0]\n",
    "\n",
    "initial_bias = np.log([train_data['label'].value_counts()[1]/train_data['label'].value_counts()[0]])\n",
    "initial_bias\n",
    "\n",
    "weight_for_0 = (1 / train_data['label'].value_counts()[0])*(len(train_data))/2.0 \n",
    "weight_for_1 = (1 / train_data['label'].value_counts()[1])*(len(train_data))/2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42g67C90Ao4J"
   },
   "outputs": [],
   "source": [
    "y_train=train_data[\"label\"].values\n",
    "y_val=val_data[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iW-iwflGAD3w"
   },
   "outputs": [],
   "source": [
    "model_dir = data_dir / 'model'\n",
    "checkpoint_path = model_dir / \"cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "batch_size=32\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10,\n",
    "                   verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
    "\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', patience = 8, verbose=1,factor=0.5, min_lr=0.000001)\n",
    "\n",
    "chkpt = ModelCheckpoint(filepath=checkpoint_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "clf = build_model()\n",
    "hist=clf.fit(datagen.flow(X_train,to_categorical(y_train),batch_size=batch_size),\n",
    "            validation_data=datagen.flow(X_val, to_categorical(y_val),batch_size=batch_size),\n",
    "            epochs=50,\n",
    "            class_weight=class_weight,\n",
    "            callbacks=[es, chkpt \n",
    "                       ,rlr\n",
    "                       ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4sW5MUOvf4sr"
   },
   "outputs": [],
   "source": [
    "train_acc = hist.history['accuracy']\n",
    "val_acc = hist.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIti_Cq2f4kN"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1,len(train_acc)+1),train_acc)\n",
    "plt.plot(range(1,len(train_acc)+1),val_acc)\n",
    "plt.legend(['train accuracy', 'validation accuracy'],loc=2)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9eaW0dkTAQ6o"
   },
   "outputs": [],
   "source": [
    "clf.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1rNA9esU0EkV"
   },
   "outputs": [],
   "source": [
    "clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uIAqEftTS8h"
   },
   "outputs": [],
   "source": [
    "loss, acc = clf.evaluate(test_datagen.flow(X_test,  to_categorical(test_data['label'].values)), verbose=2)\n",
    "print('test loss: {:0.4f}'.format(loss))\n",
    "print('test accuracy: {:5.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cross validation 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import ZeroPadding2D,Conv2D,MaxPooling2D,Flatten,Dropout,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.concatenate((X_train,X_val))\n",
    "y=np.concatenate((train_data[\"label\"].values,val_data['label'].values))\n",
    "y_tst=test_data[\"label\"].values\n",
    "print(X_train.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2():\n",
    "  model = Sequential()\n",
    "  model.add(ZeroPadding2D((1,1),input_shape=X_train.shape[1:]))\n",
    "  model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "  model.add(ZeroPadding2D((1,1)))\n",
    "  model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "  model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "  model.add(ZeroPadding2D((1,1)))\n",
    "  model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "  model.add(ZeroPadding2D((1,1)))\n",
    "  model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "  model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "  model.add(ZeroPadding2D((1,1)))\n",
    "  model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "  model.add(ZeroPadding2D((1,1)))\n",
    "  model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "  model.add(ZeroPadding2D((1,1)))\n",
    "  model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "  model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "  model.add(ZeroPadding2D((1,1)))\n",
    "  model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "  model.add(ZeroPadding2D((1,1)))\n",
    "  model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "  model.add(ZeroPadding2D((1,1)))\n",
    "  model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "  model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "  model.add(ZeroPadding2D((1,1)))\n",
    "  model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "  model.add(ZeroPadding2D((1,1)))\n",
    "  model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "  model.add(ZeroPadding2D((1,1)))\n",
    "  model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "  model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(4096, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(4096, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "  optimizer = Adam(lr = 0.0001)\n",
    "  model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=optimizer)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 더욱 깊은 모델 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class=2\n",
    "n_fold=3\n",
    "batch_size=8\n",
    "cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "tqdm_callback = tfa.callbacks.TQDMProgressBar() #jupyter notebook에서 사용하면 유용한 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = np.zeros((X_train.shape[0], n_class))\n",
    "p_tst = np.zeros((X_test.shape[0], n_class))\n",
    "for i, (i_trn, i_val) in enumerate(cv.split(X_train, y), 1):\n",
    "    print(f'training model for CV #{i}')\n",
    "    clf = build_model2()\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5,\n",
    "                       verbose=1, mode='min', baseline=None, restore_best_weights=True)\n",
    "    \n",
    "    rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n",
    "                        patience=3, min_lr=1e-6, mode='min', verbose=1)\n",
    "\n",
    "    hist=clf.fit(datagen.flow(X_train[i_trn],to_categorical(y[i_trn]),batch_size=batch_size),\n",
    "            validation_data=test_datagen.flow(X_train[i_val], to_categorical(y[i_val])),\n",
    "            epochs=50, \n",
    "            # batch_size=batch_size,\n",
    "            class_weight=class_weight,\n",
    "            callbacks=[es,\n",
    "                       rlr,\n",
    "                       # tqdm_callback #jupyter notebook에서 사용하면 유용한 기능. 상태바 출력.\n",
    "                       ]\n",
    "            )\n",
    "    p_val[i_val, :] = clf.predict(test_datagen.flow(X_train[i_val]))\n",
    "    p_tst += clf.predict(test_datagen.flow(X_test)) / n_fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(np.argmax(p_tst,axis=1),y_tst) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- momory 이슈 발생\n",
    "- 트레이닝 과정에서 확실히 더 안정적이고 좋은 성능을 보이지만 결과를 내지 못해 아쉬움."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "폐렴 분류 CNN.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
